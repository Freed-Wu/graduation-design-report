\documentclass[../main]{subfiles}
\begin{document}
\mode*

\chapter{相关工作}%
\label{cha:related-work}
\mode<presentation>{%
  \section{相关工作}%
  \label{sec:related-work}
}

\mode<article>{%
  \section{常见的多任务网络架构}%
  \label{sec:net}
}

常见的多任务网络架构分类标准有2种：

\begin{itemize}
  \item 如图~\ref{fig:hard-soft}，按参数连接方式分为硬参数连接和软参数连接，
    其中：
    \begin{itemize}
      \item 硬参数连接在所有任务的底层共享一些参数，在特定任务的顶层使用任务
        独有的参数~\cite{DBLP:conf/icml/Caruana93}。共享参数使过拟合机率降低
        至$O(n_\mathrm{task})$~\cite{Baxter1997}；
      \item 软参数连接的每个任务均有自己的参数，最后通过对不同任务的参数之间
        的差异加以约束（2 阶正则化~\cite{duong-etal-2015-low}、
        迹正则化~\cite{DBLP:journals/corr/YangH16a}等等）
        以表达相似性。
    \end{itemize}
    \begin{frame}<article>[fragile, label=hard-soft]{硬参数连接和软参数连接}
      \begin{figure}[htbp]
        \centering
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled fillcolor=gray]
              1 -> 2 -> 3
              -> {41[fillcolor=red] 42[fillcolor=blue] 43[fillcolor=green]}
              node[label="\N" shape=plaintext style=""]
              41 -> {任务1[fillcolor=red]}
              42 -> {任务2[fillcolor=blue]}
              43 -> {任务3[fillcolor=green]}
            }
          \end{dot2tex}
          \caption{硬参数连接，灰色代表的网络参数共享，三原色代表的网络参数独立}%
          \label{fig:hard}
        \end{subfigure}
        \quad
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled fillcolor=red]
              11 -> 21 -> 31 -> 41
              -> {任务1[label="\N" shape=plaintext style="" fillcolor=red]}
              node[fillcolor=blue]
              12 -> 22 -> 32 -> 42
              -> {任务2[label="\N" shape=plaintext style="" fillcolor=blue]}
              node[fillcolor=green]
              13 -> 23 -> 33 -> 43
              -> {任务3[label="\N" shape=plaintext style="" fillcolor=green]}
              edge[dir=both constraint=false]
              11 -> 12 -> 13
              21 -> 22 -> 23
              31 -> 32 -> 33
            }
          \end{dot2tex}
          \caption{软参数连接，双向箭头代表的网络参数因满足约束条件而相关}%
          \label{fig:soft}
        \end{subfigure}
        \caption{多任务网络架构按参数连接方式分类}%
        \label{fig:hard-soft}
      \end{figure}
    \end{frame}
  \item 如图~\ref{fig:encoder-decoder}，按交互集中位置分为编码器集中型和解码
    器集中型~\cite{9336293}。
    \begin{itemize}
      \item 编码器集中型只在编码器上共享参数，每个任务的解码器是相互独立的；
      \item 解码器集中型在解码阶段也会交换信息。
    \end{itemize}
    \begin{frame}<article>[fragile, label=encoder-decoder]{编码器集中型和解码
      器集中型}
      \begin{figure}[htbp]
        \centering
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled]
              {共享编码器[label="\N" fillcolor=gray]}
              -> {1[fillcolor=red] 2[fillcolor=blue] 3[fillcolor=green]}
              node[label="\N" shape=plaintext style=""]
              1 -> 任务1
              2 -> 任务2
              3 -> 任务3
            }
          \end{dot2tex}
          \caption{编码器集中型，任务交互集中在编码器}%
          \label{fig:encoder}
        \end{subfigure}
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              {
                node[label="" shape=box style=filled]
                {共享编码器[label="\N" fillcolor=gray]}
                -> {1[fillcolor=red] 2[fillcolor=blue] 3[fillcolor=green]}
                {1 2 3} ->
                {11[fillcolor=red] 22[fillcolor=blue] 33[fillcolor=green]}
              }
              node[shape=plaintext]
              11 -> 任务1
              22 -> 任务2
              33 -> 任务3
            }
          \end{dot2tex}
          \caption{解码器集中型，任务交互集中在解码器}%
          \label{fig:decoder}
        \end{subfigure}
        \caption{多任务网络架构按交互集中位置分类，其中共享编码器既可以是硬参
          数共享也可以是软参数共享}%
        \label{fig:encoder-decoder}
      \end{figure}
    \end{frame}
\end{itemize}

\newpage
常见的多任务网络架构按图~\ref{fig:encoder-decoder} 分类如图~\ref{fig:net}。

\begin{frame}[fragile]{常见的多任务学习网络架构}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=LR
        ranksep=0
        nodesep=0
        node[shape=plaintext]
        多任务网络架构 -> {编码器集中型 解码器集中型 其它}
        编码器集中型 -> {硬参数连接 十字绣网络 水闸网络 "NDDR-CNN" MTAN
        "分支 MTL"}
        解码器集中型 -> {"PAD 网络" "PAP 网络" "JTRL 网络" "MTI 网络" "PSD 网络"}
        其它 -> ASTMT
      }
    \end{dot2tex}
    \caption{常见的多任务学习网络架构}%
    \label{fig:net}
  \end{figure}
\end{frame}

图~\ref{fig:encoders} 均以图~\ref{fig:resnet} 的残差网络
ResNet~\cite{He_2016_CVPR}~\footnote{Residual Network}
为骨干。其中图~\ref{fig:nddr} 的
十字绣网络~\cite{Misra_2016_CVPR}~\footnote{Cross-stitch Network}和
区别性维度递减神经网络
NDDR-CNN~\cite{Gao_2019_CVPR}~\footnote{Neural Discriminative Dimensionality Reduction}
采用了软参数共享，而图~\ref{fig:mtan} 的
多任务注意力网络 MTAN~\cite{Liu_2019_CVPR}~\footnote{Multi-Task Attention
Network} 采用了硬参数共享。 3 种网络在 ResNet 的每个残差层前均插入一个共享的
网络（即图~\ref{fig:encoders} 中灰色代表的网络），称之为阶段（共 4 个）。在阶
段中不同的任务可以交换信息。

\begin{frame}<article>[fragile, label=resnet]{残差网络}
  \begin{figure}[htbp]
    \centering
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          node[shape=box fillcolor=lightgray]
          {
            node[style=filled]
            残差层1 残差层2 残差层3 残差层4
          }
          {输入[shape=plaintext style=""]} -> 卷积层 -> 残差层1 -> 残差层2 -> 残差层3
          -> 残差层4 -> 全局平均池化层 -> 全连接层 -> {输出[shape=plaintext style=""]}
        }
      \end{dot2tex}
      \caption{骨干}%
      \label{fig:res}
    \end{subfigure}
    \quad
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          subgraph cluster{
            label=残差层
            style=filled
            node[shape=box]
            残差块1 -> 联接1 -> 残差块2 -> 联接2
          }
          node[shape=plaintext]
          输入 -> 残差块1
          输入 -> 联接1 -> 联接2[constraint=false]
          联接2 -> 输出
        }
      \end{dot2tex}
      \caption{残差层}%
      \label{fig:reslayer}
    \end{subfigure}
    \caption{残差网络}%
    \label{fig:resnet}
  \end{figure}
\end{frame}

\begin{frame}<article>[fragile, label=encoders]{常见的以残差网络为骨干网的网络}
  \begin{figure}[htbp]
    \centering
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          {
            node[label="" shape=box style=filled fillcolor=red]
            11 12 13 14
            node[fillcolor=blue]
            21 22 23 24
            node[fillcolor=green]
            31 32 33 34
            node[shape=circle fillcolor=gray]
            102 203
          }
          node[shape=plaintext]
          任务1 任务2 任务3
          {11 21 31} -> 102 -> {12 22 32} -> 203 -> 13 -> 14 -> 任务1
          203 -> 23 -> 24 -> 任务2
          203 -> 33 -> 34 -> 任务3
        }
      \end{dot2tex}
      \caption{十字绣网络或 NDDR-CNN ，软参数共享，灰色代表的网络发生参
      数交互}%
      \label{fig:nddr}
    \end{subfigure}
    \quad
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          node[shape=plaintext]
          任务1 任务2 任务3
          node[label="" shape=box style=filled fillcolor=gray]
          1 -> 2 -> 3
          node[fillcolor=red]
          11 -> 12 -> 13 -> 14 -> 任务1
          node[fillcolor=blue]
          21 -> 22 -> 23 -> 24 -> 任务2
          node[fillcolor=green]
          31 -> 32 -> 33 -> 34 -> 任务3
          edge[constraint=false]
          1 -> {11 21 31}
          2 -> {12 22 32}
          3 -> {13 23 33}
        }
      \end{dot2tex}
      \caption{MTAN，硬参数共享}%
      \label{fig:mtan}
    \end{subfigure}
    \caption{常见的以残差网络为骨干网的网络}%
    \label{fig:encoders}
  \end{figure}
\end{frame}

如图~\ref{fig:nddr} 所示， 十字绣网络通过一个简单的线型变换$A$将每一个
阶段的输入（第$i$个任务在阶段前一个残差层的输出$x_i$联接在一起的张量$X$）变成
该阶段的输出（第$i$个任务在阶段后一个残差层的输出$\tilde{x}_i$联接在一起的张
量 $\tilde{X}$）。$A$ 是一个对角元素为$\alpha$、非对角元素为$\beta$的$T
\times T$张量，其中$T$是任务个数，$\alpha, \beta$作为超参数常取为$\alpha =
0.8, \beta = 0.05$。

\begin{align}
  \tilde{X} & = AX \\
  \label{eq:A}
  A_{ij} & = \alpha \times [i = j] + \beta \times [i \neq j] \\
  X & = \begin{bmatrix}x_1 & x_2 & \cdots & x_T\end{bmatrix}^\mathsf{T} \\
  \tilde{X} & = \begin{bmatrix}
    \tilde{x}_1 & \tilde{x}_2 & \cdots & \tilde{x}_T
  \end{bmatrix}^\mathsf{T}
\end{align}

如图~\ref{fig:nddr} 所示， NDDR-CNN 作为 十字绣网络的改进，使用 CNN
$f_\mathrm{CNN}$代替了矩阵$A$ ，并在每一个阶段对输入在任务和通道 2 个维度上进
行了信息的交换。 $f_\mathrm{CNN}$ 作为一个卷积核尺寸为$1 \times 1$、输入层数
为$TC$、输出层数为 $T$ 的卷积神经网络，其权重是一个 $T \times TC \times 1
\times 1$ 的 4 阶张量 $\tilde{A}$，初始化为$C$个$A$在第 2 个维度上的联接。$A$
由式~\ref{eq:A} 决定， $C$为每个任务的通道个数，$x_{i, j}$为第$i$个任务在阶段
前一个残差层的输出的第 $j$个通道的张量。

\begin{align}
  \tilde{X} & = f_\mathrm{CNN}(X) \\
  X & = \begin{bmatrix}x_{1, 1} & x_{1, 2} & \cdots & x_{T, C}\end{bmatrix}^\mathsf{T}
\end{align}

\newpage
\begin{frame}<article>[fragile, label=mtan-detail]{MTAN 的 ResNet 骨干与任务1
  信息交换的细节}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=BT
        ranksep=0
        node[shape=box]
        subgraph cluster{
          label=残差层
          style=filled
          残差块1 -> 联接1 -> 残差块2 -> 联接2
        }
        subgraph cluster1{
          label=任务1阶段k
          style=filled
          fillcolor=red
          联接3 -> 注意层 -> 联接4 -> 提纯层
        }
        node[shape=plaintext]
        输入 -> 残差块1
        任务1阶段k输入 -> 联接3
        联接2 -> 输出
        提纯层 -> 任务1阶段k输出
        edge[constraint=false]
        输入 -> 联接1 -> {联接2 联接3}
        联接2 -> 联接4
      }
    \end{dot2tex}
    \caption{MTAN 的 ResNet 骨干与任务1 信息交换的细节}%
    \label{fig:mtan-detail}
  \end{figure}
\end{frame}

因为图~\ref{fig:reslayer} 的 ResNet 的每个残差层均由 2 个残差块组成，所以可以
利用残差块再进一步增加任务间信息交换的次数。如图~\ref{fig:mtan-detail} 所示，
对每个任务，将残差块2前的输出$x^k$与阶段$k$的输入$y^{k - 1}$联接（在第 1 个阶
段 $y^0$不存在不用联接）后，经过一个注意层（由2个 CNN 组成的网络）后得到
$\tilde{y}^{k - 1}$，与残差块2后的输出$x^k$联接后通过一个残差块（该操作被称为
提纯）后得到$y^k$。重复以上过程经过 4 个阶段再解码后得到第$i$个任务的最终输出
。

\mode<article>{%
  \section{常见的多任务优化方法}%
  \label{sec:optimize}
}

常见的多任务优化方法如图~\ref{fig:optimize}。其中，任务平衡指：

\begin{definition}[任务平衡]
  对由$n$个任务组成的多任务模型，每个任务损失$L_i$加权得到总损失$L = \sum_{i
  = 1}^n\gamma_i L_i$，寻找合适的$\gamma_i$避免因某一任务损失过大导致其它任务损
  失变化对总损失的影响忽略不计的现象发生。
\end{definition}

\begin{frame}<article>[fragile, label=optimize]{常见的多任务优化方法}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=LR
        ranksep=0
        nodesep=0
        node[shape=plaintext]
        多任务优化方法 -> {任务平衡 其它}
        任务平衡 -> {固定权重 根据不确定性分配权重 梯度正则化 动态加权平均
        动态任务优先级 多目标优化}
        其它 -> {基于对抗 模块化 启发式 梯度符号随机失活}
      }
    \end{dot2tex}
    \caption{常见的多任务优化方法}%
    \label{fig:optimize}
  \end{figure}
\end{frame}

\subsection{固定权重}%
\label{sub:fixed}

传统的固定权重将权重视为一个超参数，通过手工调参或网格搜索来寻找合适的权重，
存在时间成本高、不可调整的缺点。更好的权重应该是动态的，随着学习的阶段、效果
、难易进行调整。

\subsection{根据不确定性分配权重}%
\label{sub:uncertainty}

对于某个回归任务，其真正条件概率$y$服从式~\ref{eq:p}，以模型预测的$f^w(x)$为
均值，以任务依赖型不确定性$\sigma$为方差的正态分布\cite{Kendall_2018_CVPR}。

\begin{align}
  \label{eq:p}
  p(y|f^W(x)) = \frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{{(y - f^W(x))}^2}{2\sigma^2}
\end{align}

对于多个回归任务，则有式~\ref{eq:ps}：

\begin{align}
  \label{eq:ps}
  p(y_1, y_2, \ldots, y_k|f^W(x)) = p(y_1|f^W(x))p(y_2|f^W(x)) \ldots p(y_k|f^W(x))
\end{align}

考虑让简单（出现概率更高）的任务具有更高的权重，令损失函数正比于负对数似然，
求式~\ref{eq:log}最小值。

\begin{align}
  \label{eq:log}
  L(W, \sigma_{1, 2, \ldots, n}) \propto -\log p(y_1, y_2, \ldots, y_k|f^W(x))
\end{align}

最终得到基于不确定性的优化方法式~\ref{eq:uncertainty}：

\begin{align}
  \label{eq:uncertainty}
  L(W, \sigma_{1, 2, \ldots, n}) \propto \sum_{k = 1}^n(\frac{1}{2\sigma_k^2}L_k(W) + \log\sigma_k)
\end{align}

\subsection{梯度正则化}%
\label{sub:gradnorm}

\cite{DBLP:journals/corr/abs-1711-02257}除了标签损失外定义真实梯度$G_W^i(t)$
和理想梯度$\bar{G}_W(t) \times {(r_i(t))}^\alpha$的距离为梯度损失
式~\ref{eq:grad}用来衡量权重的好坏。
$\bar{G}_W(t)$是所有任务梯度标准化值的期望，$r_i(t)$是任务的相对反向训练速度
。这两种损失独立优化不会加权求和。

\begin{align}
  \label{eq:grad}
  L(t; w_i(t)) & = \sum_i||G_W^i(t) - \bar{G}_W(t) \times {(r_i(t))}^\alpha||_1 \\
  G_W^i(t) & = || \nabla_W w_i(t)L_i(t) ||_2 \\
  \bar{G}_W(t) & = E G_W^i(t) \\
  \tilde{L}_i(t) & = L_i(t)/L_i(0) \\
  r_i(t) & = \tilde{L}_i(t)/E \tilde{L}_i(t)
\end{align}

梯度正则化同时考虑了损失和训练速度的量级，但计算梯度需要消耗大量时间，而且
$L_i(0)$带有一定的随机性，会导致重复实验结果存在差别。

\subsection{动态加权平均}%
\label{sub:dwa}

\cite{Liu_2019_CVPR}将第$i$个任务在第$t$步的权重$w_i$取为式~\ref{eq:w}。其中
，$N$是任务个数，$T$是一个被称为温度的超参数。

\begin{align}
  \label{eq:w}
  w_i(t) & = \frac{N\exp(r_i(t - 1)/T)}{\sum_n\exp(r_n(t - 1)/T)} \\
  r_n(t - 1) & = \frac{L_n(t - 1)}{L_n(t - 2)}
\end{align}

动态加权平均计算速度快，但忽视了损失的量级。

\subsection{动态任务优先级}%
\label{sub:dtp}

\cite{Guo_2018_ECCV}希望让难度更高的任务具有更高的权重$w_i(t)$。
如式~\ref{eq:dtp}，$k_i(t) \in [0, 1]$是每个任务容易的衡量指标，例如分类的准
确率，$\gamma_i$则为超参数。

\begin{align}
  \label{eq:dtp}
  w_i(t) = -{(1 - k_i(t))}^\gamma_i\log k_i(t)
\end{align}

动态任务优先级计算速度快，但也忽视了损失的量级。

\subsection{多目标优化}%
\label{sub:pareto}

\cite{NEURIPS2019_685bfde0}将任务平衡问题作为一个多目标优化问题考虑，寻找该问
题的 Pareto 前沿。任务的权重即为多目标优化的决策向量，任务损失即为优化目标函
数。

\begin{definition}[Pareto 占优]
  若某一决策向量$w_1, w_2$满足式~\ref{eq:pareto}，称$w_1$优于$w_2$，其中$f_i$
  为优化目标函数，优化目标为使$f_i(w)$最小。
\end{definition}

\begin{align}
  \label{eq:pareto}
  \{\forall i f_i(w_1) \leqslant f_i(w_2)\} \land
  \{\exists j f_i(w_1) < f_i(w_2)\}
\end{align}

\begin{definition}[Pareto 最优]
  对某一个决策向量，在参数空间里不存在任何决策向量优于它。
\end{definition}

\begin{definition}[Pareto 前沿]
  所有 Pareto 最优解构成的集合。
\end{definition}

\mode<all>
\end{document}
