\documentclass[../main]{subfiles}
\begin{document}
\mode*

\chapter{相关工作}%
\label{cha:related-work}
\mode<presentation>{%
  \section{相关工作}%
  \label{sec:related-work}
}

\mode<article>{%
  \section{常见的多任务网络架构}%
  \label{sec:net}
}

常见的多任务网络架构分类标准有2种：

\begin{itemize}
  \item 如图~\ref{fig:hard-soft}，按参数连接方式分为硬参数连接和软参数连接，
    其中：
    \begin{itemize}
      \item 硬参数连接在所有任务的底层共享一些参数，在特定任务的顶层使用任务
        独有的参数~\cite{DBLP:conf/icml/Caruana93}。共享参数使过拟合机率降低
        至$O(n_\mathrm{task})$~\cite{Baxter1997}；
      \item 软参数连接的每个任务均有自己的参数，最后通过对不同任务的参数之间
        的差异加以约束（2 阶正则化~\cite{duong-etal-2015-low}、
        迹正则化~\cite{DBLP:journals/corr/YangH16a}等等）
        以表达相似性。
    \end{itemize}
    \begin{frame}<article>[fragile, label=hard-soft]{硬参数连接和软参数连接}
      \begin{figure}[htbp]
        \centering
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled fillcolor=gray]
              1 -> 2 -> 3
              -> {41[fillcolor=red] 42[fillcolor=blue] 43[fillcolor=green]}
              node[label="\N" shape=plaintext style=""]
              41 -> {任务1[fillcolor=red]}
              42 -> {任务2[fillcolor=blue]}
              43 -> {任务3[fillcolor=green]}
            }
          \end{dot2tex}
          \caption{硬参数连接，灰色代表的网络参数共享，三原色代表的网络参数独立}%
          \label{fig:hard}
        \end{subfigure}
        \quad
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled fillcolor=red]
              11 -> 21 -> 31 -> 41
              -> {任务1[label="\N" shape=plaintext style="" fillcolor=red]}
              node[fillcolor=blue]
              12 -> 22 -> 32 -> 42
              -> {任务2[label="\N" shape=plaintext style="" fillcolor=blue]}
              node[fillcolor=green]
              13 -> 23 -> 33 -> 43
              -> {任务3[label="\N" shape=plaintext style="" fillcolor=green]}
              edge[dir=both constraint=false]
              11 -> 12 -> 13
              21 -> 22 -> 23
              31 -> 32 -> 33
            }
          \end{dot2tex}
          \caption{软参数连接，双向箭头代表的网络参数因满足约束条件而相关}%
          \label{fig:soft}
        \end{subfigure}
        \caption{多任务网络架构按参数连接方式分类}%
        \label{fig:hard-soft}
      \end{figure}
    \end{frame}
  \item 如图~\ref{fig:encoder-decoder}，按交互集中位置分为编码器集中型和解码
    器集中型~\cite{9336293}。
    \begin{itemize}
      \item 编码器集中型只在编码器上共享参数，每个任务的解码器是相互独立的；
      \item 解码器集中型在解码阶段也会交换信息。
    \end{itemize}
    \begin{frame}<article>[fragile, label=encoder-decoder]{编码器集中型和解码
      器集中型}
      \begin{figure}[htbp]
        \centering
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              node[label="" shape=box style=filled]
              {共享编码器[label="\N" fillcolor=gray]}
              -> {1[fillcolor=red] 2[fillcolor=blue] 3[fillcolor=green]}
              node[label="\N" shape=plaintext style=""]
              1 -> 任务1
              2 -> 任务2
              3 -> 任务3
            }
          \end{dot2tex}
          \caption{编码器集中型，任务交互集中在编码器}%
          \label{fig:encoder}
        \end{subfigure}
        \begin{subfigure}[htbp]{0.45\linewidth}
          \centering
          \begin{dot2tex}[scale=\scale]
            digraph G{
              rankdir=BT
              ranksep=0
              {
                node[label="" shape=box style=filled]
                {共享编码器[label="\N" fillcolor=gray]}
                -> {1[fillcolor=red] 2[fillcolor=blue] 3[fillcolor=green]}
                {1 2 3} ->
                {11[fillcolor=red] 22[fillcolor=blue] 33[fillcolor=green]}
              }
              node[shape=plaintext]
              11 -> 任务1
              22 -> 任务2
              33 -> 任务3
            }
          \end{dot2tex}
          \caption{解码器集中型，任务交互集中在解码器}%
          \label{fig:decoder}
        \end{subfigure}
        \caption{多任务网络架构按交互集中位置分类，其中共享编码器既可以是硬参
          数共享也可以是软参数共享}%
        \label{fig:encoder-decoder}
      \end{figure}
    \end{frame}
\end{itemize}

常见的多任务网络架构按图~\ref{fig:encoder-decoder} 分类如图~\ref{fig:net}。

\begin{frame}<article>[fragile, label=net]{常见的多任务学习网络架构}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=LR
        ranksep=0
        nodesep=0
        node[shape=plaintext]
        多任务网络架构 -> {编码器集中型 解码器集中型 其它}
        编码器集中型 -> {硬参数连接 十字绣网络 水闸网络 "NDRR-CNN" MTAN
        "分支 MTL"}
        解码器集中型 -> {"PAD 网络" "PAP 网络" "JTRL 网络" "MTI 网络" "PSD 网络"}
        其它 -> ASTMT
      }
    \end{dot2tex}
    \caption{常见的多任务学习网络架构}%
    \label{fig:net}
  \end{figure}
\end{frame}

\subsection{常见的编码器集中型}%
\label{sub:encoders}

常见的编码器集中型网络都以图~\ref{fig:resnet} 的 ResNet~\cite{He_2016_CVPR}
为骨干。其中图~\ref{fig:nddr} 的十字绣网络~\cite{Misra_2016_CVPR}和
NDDR-CNN~\cite{Gao_2019_CVPR} 采用了软参数共享，而图~\ref{fig:mtan} 的
MTAN~\cite{Liu_2019_CVPR} 采用了硬参数共享。 3 种网络在 ResNet 的每个残差层前
均插入一个共享的网络（即图~\ref{fig:encoders} 中灰色代表的网络），称之为阶段
（共 4 个）。在阶段中不同的任务可以交换信息。

如图~\ref{fig:nddr} 所示， 十字绣网络通过一个简单的线型变换$A$将每一个
阶段的输入（第$i$个任务在阶段前一个残差层的输出$x_i$联接在一起的张量$X$）变成
该阶段的输出（第$i$个任务在阶段后一个残差层的输出$\tilde{x}_i$联接在一起的张
量 $\tilde{X}$）。$A$ 是一个对角元素为$\alpha$、非对角元素为$\beta$的$T
\times T$张量，其中$T$是任务个数，$\alpha, \beta$作为超参数常取为$\alpha =
0.8, \beta = 0.05$。

\begin{align}
  \tilde{X} & = AX \\
  \label{eq:A}
  A_{ij} & = \alpha \times [i = j] + \beta \times [i \neq j] \\
  X & = \begin{bmatrix}x_1 & x_2 & \cdots & x_T\end{bmatrix}^\mathsf{T} \\
  \tilde{X} & = \begin{bmatrix}
    \tilde{x}_1 & \tilde{x}_2 & \cdots & \tilde{x}_T
  \end{bmatrix}^\mathsf{T}
\end{align}

如图~\ref{fig:nddr} 所示， NDRR-CNN 作为 十字绣网络的改进，使用 CNN
$f_\mathrm{CNN}$代替了矩阵$A$ ，并在每一个阶段对输入在任务和通道 2 个维度上进
行了信息的交换。 $f_\mathrm{CNN}$ 作为一个卷积核尺寸为$1 \times 1$、输入层数
为$TC$、输出层数为 $T$ 的卷积神经网络，其权重是一个 $T \times TC \times 1
\times 1$ 的 4 阶张量 $\tilde{A}$，初始化为$C$个$A$在第 2 个维度上的联接。$A$
由式~\ref{eq:A} 决定， $C$为每个任务的通道个数，$x_{i, j}$为第$i$个任务在阶段
前一个残差层的输出的第 $j$个通道的张量。

\begin{align}
  \tilde{X} & = f_\mathrm{CNN}(X) \\
  X & = \begin{bmatrix}x_{1, 1} & x_{1, 2} & \cdots & x_{T, C}\end{bmatrix}^\mathsf{T}
\end{align}

因为图~\ref{fig:reslayer} 的 ResNet 的每个残差层均由 2 个残差块组成，所以可以
利用残差块再进一步增加任务间信息交换的次数。如图~\ref{fig:mtan-detail} 所示，
对每个任务，将残差块2前的输出$x^k$与阶段$k$的输入$y^{k - 1}$联接（在第 1 个阶
段 $y^0$不存在不用联接）后，经过一个注意层（由2个 CNN 组成的网络）后得到
$\tilde{y}^{k - 1}$，与残差块2后的输出$x^k$联接后通过一个残差块（该操作被称为
提纯）后得到$y^k$。重复以上过程经过 4 个阶段再解码后得到第$i$个任务的最终输出
。

\begin{frame}<article>[fragile, label=encoders]{常见的编码器集中型}
  \begin{figure}[htbp]
    \centering
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          {
            node[label="" shape=box style=filled fillcolor=red]
            11 12 13 14
            node[fillcolor=blue]
            21 22 23 24
            node[fillcolor=green]
            31 32 33 34
            node[shape=circle fillcolor=gray]
            102 203
          }
          node[shape=plaintext]
          任务1 任务2 任务3
          {11 21 31} -> 102 -> {12 22 32} -> 203 -> 13 -> 14 -> 任务1
          203 -> 23 -> 24 -> 任务2
          203 -> 33 -> 34 -> 任务3
        }
      \end{dot2tex}
      \caption{十字绣网络或 NDDR-CNN ，软参数共享，灰色代表的网络发生参
      数交互}%
      \label{fig:nddr}
    \end{subfigure}
    \quad
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          node[shape=plaintext]
          任务1 任务2 任务3
          node[label="" shape=box style=filled fillcolor=gray]
          1 -> 2 -> 3
          node[fillcolor=red]
          11 -> 12 -> 13 -> 14 -> 任务1
          node[fillcolor=blue]
          21 -> 22 -> 23 -> 24 -> 任务2
          node[fillcolor=green]
          31 -> 32 -> 33 -> 34 -> 任务3
          edge[constraint=false]
          1 -> {11 21 31}
          2 -> {12 22 32}
          3 -> {13 23 33}
        }
      \end{dot2tex}
      \caption{MTAN，硬参数共享}%
      \label{fig:mtan}
    \end{subfigure}
    \caption{常见的编码器集中型}%
    \label{fig:encoders}
  \end{figure}
\end{frame}

\begin{frame}<article>[fragile, label=mtan-detail]{MTAN 的 ResNet 骨干与任务1
  信息交换的细节}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=BT
        ranksep=0
        node[shape=box]
        subgraph cluster{
          label=残差层
          style=filled
          残差块1 -> 联接1 -> 残差块2 -> 联接2
        }
        subgraph cluster1{
          label=任务1阶段k
          style=filled
          fillcolor=red
          联接3 -> 注意层 -> 联接4 -> 提纯层
        }
        node[shape=plaintext]
        输入 -> 残差块1
        任务1阶段k输入 -> 联接3
        联接2 -> 输出
        提纯层 -> 任务1阶段k输出
        edge[constraint=false]
        输入 -> 联接1 -> {联接2 联接3}
        联接2 -> 联接4
      }
    \end{dot2tex}
    \caption{MTAN 的 ResNet 骨干与任务1 信息交换的细节}%
    \label{fig:mtan-detail}
  \end{figure}
\end{frame}

\begin{frame}<article>[fragile, label=resnet]{残差网络}
  \begin{figure}[htbp]
    \centering
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          node[shape=box fillcolor=lightgray]
          {
            node[style=filled]
            残差层1 残差层2 残差层3 残差层4
          }
          {输入[shape=plaintext style=""]} -> 卷积层 -> 残差层1 -> 残差层2 -> 残差层3
          -> 残差层4 -> 全局平均池化层 -> 全连接层 -> {输出[shape=plaintext style=""]}
        }
      \end{dot2tex}
      \caption{骨干}%
      \label{fig:res}
    \end{subfigure}
    \quad
    \begin{subfigure}[htbp]{0.45\linewidth}
      \centering
      \begin{dot2tex}[scale=\scale]
        digraph G{
          rankdir=BT
          ranksep=0
          subgraph cluster{
            label=残差层
            style=filled
            node[shape=box]
            残差块1 -> 联接1 -> 残差块2 -> 联接2
          }
          node[shape=plaintext]
          输入 -> 残差块1
          输入 -> 联接1 -> 联接2[constraint=false]
          联接2 -> 输出
        }
      \end{dot2tex}
      \caption{残差层}%
      \label{fig:reslayer}
    \end{subfigure}
    \caption{残差网络}%
    \label{fig:resnet}
  \end{figure}
\end{frame}

\subsection{常见的解码器集中型}%
\label{sub:decoders}

\begin{frame}{HRNet}
  MTI 网络~\cite{vandenhende2020mti}使用高分辨率网络
  HRNet~\cite{WangSCJDZLMTWLX19} 做骨干而不是 ResNet。图~\ref{fig:hrnet} 的
  HRNet 抛弃了分辨率从高到低，最后再插值升高的串联卷积网络的架构，将多个不同分
  辨率的特征并行混合以充分利用不同分辨率的特征。
\end{frame}

网络层的设计如下：

\begin{description}
  \item[残差层]残差层与图~\ref{fig:reslayer} 相同。每个残差层后通过步距为 2
    的卷积进行降采样得到一个分辨率减半的张量用于新分支网络的输入，这会导致最
    终的多个输出存在分辨率依次减半的关系；
  \item[混合层]混合层将多个输入通过一个卷积层后相加得到输出，如果输入的分辨率
    （图片以像素为单位的长、宽）不同则先进行双线性插值；
  \item[过渡层]如果前后输入输出通道数相同，则过渡层为恒等变换，否则过渡层为一
    个卷积层。
\end{description}

\begin{frame}[fragile]{HRNet}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      strict digraph G{
        rankdir=BT
        ranksep=0
        style=invis
        node[shape=box]
        subgraph cluster{
          {输入[shape=plaintext]} -> 残差层 ->
          过渡层1a -> 残差层1a -> 混合层1a ->
          过渡层2a -> 残差层2a -> 混合层2a ->
          过渡层3a -> 残差层3a -> 混合层3a ->
          {输出a[shape=plaintext]}
        }
        subgraph cluster1{
          过渡层1b -> 残差层1b -> 混合层1b ->
          过渡层2b -> 残差层2b -> 混合层2b ->
          过渡层3b -> 残差层3b -> 混合层3b ->
          {输出b[shape=plaintext]}
        }
        subgraph cluster2{
          过渡层2c -> 残差层2c -> 混合层2c ->
          过渡层3c -> 残差层3c -> 混合层3c ->
          {输出c[shape=plaintext]}
        }
        subgraph cluster3{
          过渡层3d -> 残差层3d -> 混合层3d ->
          {输出d[shape=plaintext]}
        }
        {残差层1a 残差层1b} -> {混合层1a 混合层1b}
        {残差层2a 残差层2b 残差层2c} -> {混合层2a 混合层2b 混合层2c}
        {残差层3a 残差层3b 残差层3c 残差层3d} ->
        {混合层3a 混合层3b 混合层3c 混合层3d}
        edge[style=dashed]
        残差层 -> 过渡层1b
        混合层1b -> 过渡层2c
        混合层2c -> 过渡层3d
      }
    \end{dot2tex}
    \caption{HRNet ，虚线箭头代表利用步距为2的卷积进行降采样，最后多个输出的分
      辨率因降采样的原因依次减半}%
    \label{fig:hrnet}
  \end{figure}
\end{frame}

\begin{frame}{MTI 网络}
  MTI 网络如图~\ref{fig:mti}。解码器集中型网络的任务交互集中在解码器，所以在骨
  干上取消了阶段的设计，改为在解码器上进行任务信息的交换。
\end{frame}

\begin{frame}[fragile]{MTI 网络}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=BT
        ranksep=0
        nodesep=0
        node[shape=plaintext]
        {HRNet[shape=box]} -> {输出a 输出b 输出c 输出d}
        subgraph cluster{
          node[shape=box]
          label=解码器
          {
            style=invis
            subgraph cluster1{
              混合a 混合b 混合c 混合d
            }
            subgraph cluster2{
              初始化a 初始化b 初始化c 初始化d
            }
          }
          混合a -> 初始化a
          混合b -> 初始化b
          混合c -> 初始化c
          混合d -> 初始化d
          {
            edge[style=dashed]
            初始化a -> 蒸馏层a
            初始化b -> 蒸馏层b
            初始化c -> 蒸馏层c
            初始化d -> 蒸馏层d
          }
          {蒸馏层a 蒸馏层b 蒸馏层c 蒸馏层d} -> {特征聚合1 特征聚合2 特征聚合3}
          edge[dir=back]
          混合b -> 特征传播层b
          混合c -> 特征传播层c
          混合d -> 特征传播层d
          edge[style=dashed]
          特征传播层b -> 初始化a
          特征传播层c -> 初始化b
          特征传播层d -> 初始化c
        }
        输出a -> 混合a
        输出b -> 混合b
        输出c -> 混合c
        输出d -> 混合d
        特征聚合1 -> 任务1输出
        特征聚合2 -> 任务2输出
        特征聚合3 -> 任务3输出
      }
    \end{dot2tex}
    \caption{MTI ，虚线箭头代表输出为与任务个数相同的张量}%
    \label{fig:mti}
  \end{figure}
\end{frame}

\mode<article>{%
  \section{常见的多任务优化方法}%
  \label{sec:optimize}
}

常见的多任务优化方法如图~\ref{fig:optimize}。其中，任务平衡指：

\begin{definition}[任务平衡]
  对由$n$个任务组成的多任务模型，每个任务损失$L_i$加权得到总损失$L = \sum_{i
  = 1}^n\gamma_i L_i$，寻找合适的$\gamma_i$避免因某一任务损失过大导致其它任务损
  失变化对总损失的影响忽略不计的现象发生。
\end{definition}

\begin{frame}<article>[fragile, label=optimize]{常见的多任务优化方法}
  \begin{figure}[htbp]
    \centering
    \begin{dot2tex}[scale=\scale]
      digraph G{
        rankdir=LR
        ranksep=0
        nodesep=0
        node[shape=plaintext]
        多任务优化方法 -> {任务平衡 其它}
        任务平衡 -> {固定权重 根据不确定性分配权重 梯度正则化 动态加权平均
        动态任务优先级 多目标优化}
        其它 -> {基于对抗 模块化 启发式 梯度符号随机失活}
      }
    \end{dot2tex}
    \caption{常见的多任务优化方法}%
    \label{fig:optimize}
  \end{figure}
\end{frame}

\subsection{固定权重}%
\label{sub:fixed}

传统的固定权重将权重视为一个超参数，通过手工调参或网格搜索来寻找合适的权重，
存在时间成本高、不可调整的缺点。更好的权重应该是动态的，随着学习的阶段、效果
、难易进行调整。

\subsection{根据不确定性分配权重}%
\label{sub:uncertainty}

对于某个回归任务，其真正条件概率$y$服从式~\ref{eq:p}，以模型预测的$f^w(x)$为
均值，以任务依赖型不确定性$\sigma$为方差的正态分布\cite{Kendall_2018_CVPR}。

\begin{align}
  \label{eq:p}
  p(y|f^W(x)) = \frac{1}{\sqrt{2\pi}\sigma}\exp-\frac{{(y - f^W(x))}^2}{2\sigma^2}
\end{align}

对于多个回归任务，则有式~\ref{eq:ps}：

\begin{align}
  \label{eq:ps}
  p(y_1, y_2, \ldots, y_k|f^W(x)) = p(y_1|f^W(x))p(y_2|f^W(x)) \ldots p(y_k|f^W(x))
\end{align}

考虑让简单（出现概率更高）的任务具有更高的权重，令损失函数正比于负对数似然，
求式~\ref{eq:log}最小值。

\begin{align}
  \label{eq:log}
  L(W, \sigma_{1, 2, \ldots, n}) \propto -\log p(y_1, y_2, \ldots, y_k|f^W(x))
\end{align}

最终得到基于不确定性的优化方法式~\ref{eq:uncertainty}：

\begin{align}
  \label{eq:uncertainty}
  L(W, \sigma_{1, 2, \ldots, n}) \propto \sum_{k = 1}^n(\frac{1}{2\sigma_k^2}L_k(W) + \log\sigma_k)
\end{align}

\subsection{梯度正则化}%
\label{sub:gradnorm}

\cite{DBLP:journals/corr/abs-1711-02257}除了标签损失外定义真实梯度$G_W^i(t)$
和理想梯度$\bar{G}_W(t) \times {(r_i(t))}^\alpha$的距离为梯度损失
式~\ref{eq:grad}用来衡量权重的好坏。
$\bar{G}_W(t)$是所有任务梯度标准化值的期望，$r_i(t)$是任务的相对反向训练速度
。这两种损失独立优化不会加权求和。

\begin{align}
  \label{eq:grad}
  L(t; w_i(t)) & = \sum_i||G_W^i(t) - \bar{G}_W(t) \times {(r_i(t))}^\alpha||_1 \\
  G_W^i(t) & = || \nabla_W w_i(t)L_i(t) ||_2 \\
  \bar{G}_W(t) & = E G_W^i(t) \\
  \tilde{L}_i(t) & = L_i(t)/L_i(0) \\
  r_i(t) & = \tilde{L}_i(t)/E \tilde{L}_i(t)
\end{align}

梯度正则化同时考虑了损失和训练速度的量级，但计算梯度需要消耗大量时间，而且
$L_i(0)$带有一定的随机性，会导致重复实验结果存在差别。

\subsection{动态加权平均}%
\label{sub:dwa}

\cite{Liu_2019_CVPR}将第$i$个任务在第$t$步的权重$w_i$取为式~\ref{eq:w}。其中
，$N$是任务个数，$T$是一个被称为温度的超参数。

\begin{align}
  \label{eq:w}
  w_i(t) & = \frac{N\exp(r_i(t - 1)/T)}{\sum_n\exp(r_n(t - 1)/T)} \\
  r_n(t - 1) & = \frac{L_n(t - 1)}{L_n(t - 2)}
\end{align}

动态加权平均计算速度快，但忽视了损失的量级。

\subsection{动态任务优先级}%
\label{sub:dtp}

\cite{Guo_2018_ECCV}希望让难度更高的任务具有更高的权重$w_i(t)$。
如式~\ref{eq:dtp}，$k_i(t) \in [0, 1]$是每个任务容易的衡量指标，例如分类的准
确率，$\gamma_i$则为超参数。

\begin{align}
  \label{eq:dtp}
  w_i(t) = -{(1 - k_i(t))}^\gamma_i\log k_i(t)
\end{align}

动态任务优先级计算速度快，但也忽视了损失的量级。

\subsection{多目标优化}%
\label{sub:pareto}

\cite{NEURIPS2019_685bfde0}将任务平衡问题作为一个多目标优化问题考虑，寻找该问
题的 Pareto 前沿。任务的权重即为多目标优化的决策向量，任务损失即为优化目标函
数。

\begin{definition}[Pareto 占优]
  若某一决策向量$w_1, w_2$满足式~\ref{eq:pareto}，称$w_1$优于$w_2$，其中$f_i$
  为优化目标函数，优化目标为使$f_i(w)$最小。
\end{definition}

\begin{align}
  \label{eq:pareto}
  \{\forall i f_i(w_1) \leqslant f_i(w_2)\} \land
  \{\exists j f_i(w_1) < f_i(w_2)\}
\end{align}

\begin{definition}[Pareto 最优]
  对某一个决策向量，在参数空间里不存在任何决策向量优于它。
\end{definition}

\begin{definition}[Pareto 前沿]
  所有 Pareto 最优解构成的集合。
\end{definition}

\mode<all>
\end{document}
